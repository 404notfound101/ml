{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment C",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vw6FR1FUfD1"
      },
      "source": [
        "# 1.0 Introduction to JAX\n",
        "Main references:\n",
        "- [1] https://github.com/google/jax\n",
        "\n",
        "- [2] https://colindcarroll.com/2019/04/06/exercises-in-automatic-differentiation-using-autograd-and-jax/\n",
        "\n",
        "- [3] https://github.com/HIPS/autograd/blob/master/docs/tutorial.md\n",
        "- [4] https://colinraffel.com/blog/you-don-t-know-jax.html\n",
        "- [5] https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html\n",
        "\n",
        "\n",
        "*At its core, JAX is an extensible system for transforming numerical functions.* JAX implements an updated version of **autograd**. Autograd's `grad` function takes in a **scalar** function, and returns to you the gradient function. \n",
        "\n",
        " In this tutorial we will go through some of the main features included in the JAX package: ```grad``` and ```jit```.\n",
        "\n",
        " One of the main features you will probably use during this course is the ```grad``` function for computing gradients of your loss functions with respect to your model's weights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnUOTQXHqMfv"
      },
      "source": [
        "## 1.1 Step-by-step example on how to use ```jax.grad``` for auto-differentiation.\n",
        "\n",
        "**1) Define your function**\n",
        "\n",
        "$$y = \\sigma(wx + b),\\ \\sigma(x)=\\frac{1}{1+e^{-x}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4l38DEGQUitN"
      },
      "source": [
        "import jax.numpy as np\n",
        "from jax import grad, jit, vmap, random, value_and_grad\n",
        "import itertools\n",
        "import time\n",
        "\n",
        "\n",
        "key = random.PRNGKey(1)\n",
        "\n",
        "x = random.normal(key)\n",
        "w = random.normal(key+1)\n",
        "b = random.normal(key+2)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "# \n",
        "def make_linear_sigmoid(x): \n",
        "  def predict(W, b): # Here, you define the function with the parameters that define your model\n",
        "    return sigmoid(np.dot(x, W) + b)\n",
        "  return predict "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6F1D2CpUvDK"
      },
      "source": [
        "## **2) Define your gradient with respect to the fitting weights**\n",
        "\n",
        "\n",
        "The function `grad` takes as arguments:\n",
        "-  `fun`: the numpy **function** for which the computation of the gradients is needed.\n",
        "- `argnums`: the **arguments** of the functions  with respect to which the function will be differentiated\n",
        "\n",
        "```\n",
        "grad_not_jit(fun = make_linear_sigmoid(x),\n",
        "                              argnums =  (0,1))\n",
        "\n",
        "```\n",
        "Returns the evaluated **gradients**.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Quick quiz**: what would you change in the code above if you needed to differentiate the function only with respect to $b$?\n",
        "a: argnums=(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSKXLHE0U2p2"
      },
      "source": [
        "**Using jit to speed up functions (VERY IMPORTANT FOR THE FUTURE)**\n",
        "\n",
        "JAX provides jit (just in time compiler) which takes Python (using numpy) functions and compiles them such that they can be run efficiently on the chosen accelerators (CPU/GPU/TPU). Using jit could significantly speed up your computations and it requires little to no overhead in your coding. Let's have a look at a simple example. It is sufficient to use the jit decorator in front of your function such that the declared operation are compiled in advance (only once) and the code will run much more efficiently without any interpreter overhead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlNiA1psU4u6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb07c29-d158-4eeb-d699-9e59f1030d4f"
      },
      "source": [
        "# I am ignoring the values returned for timing because of issues with the scoping of functions (timeit issue)\n",
        "grad_not_jit = grad(make_linear_sigmoid(x), (0,1))\n",
        "%timeit _, _ = grad_not_jit(w,b) # AUTOGRAD\n",
        "\n",
        "grad_jit = jit(grad(make_linear_sigmoid(x), (0,1)))\n",
        "%timeit _, _ = grad_jit(w,b) # AUTOGRAD\n",
        "\n",
        "# Check with the analytical computation\n",
        "w_gradient , b_gradient = grad_jit(w,b) # AUTOGRAD\n",
        "\n",
        "w_gradient_manual = sigmoid(np.dot(x, w) + b) * (1 - sigmoid(np.dot(x, w) + b)) * x\n",
        "b_gradient_manual = sigmoid(np.dot(x, w) + b) * (1 - sigmoid(np.dot(x, w) + b)) \n",
        "\n",
        "print()\n",
        "print('Autograd result : w')\n",
        "print(w_gradient)\n",
        "print('Manually derived result : w')\n",
        "print(w_gradient_manual)\n",
        "\n",
        "print('Autograd result : b')\n",
        "print(b_gradient)\n",
        "print('Manually derived result : b')\n",
        "print(b_gradient_manual)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The slowest run took 79.79 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "1 loop, best of 5: 5.73 ms per loop\n",
            "The slowest run took 1772.05 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
            "10000 loops, best of 5: 71.6 Âµs per loop\n",
            "\n",
            "Autograd result : w\n",
            "-0.26804116\n",
            "Manually derived result : w\n",
            "-0.2680411\n",
            "Autograd result : b\n",
            "0.22633177\n",
            "Manually derived result : b\n",
            "0.22633173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zafRRNriqeyq"
      },
      "source": [
        "#2.0 Review of Linear Regression\n",
        "As studied in your lecture, in linear regression, we are given a data set $\\mathcal{D} =  \\{(x_n,t_n)\\}_{n=1}^N$, where $x_n = \\mathbb{R}^d$, where $d \\geq 1$ represents the dimension of your data,  and $t_n \\in \\mathbb{R}$ which are the target values that each data $x_n$ corresponds to. \n",
        "\n",
        "Next, we seek a prediction function $y$ that takes in $x_n$ and outputs a prediction vector,\n",
        "$$\n",
        "  y(x) = \\textstyle\\sum\\limits_{i = 1}^d w_i x_i + b.\n",
        "$$  \n",
        "\n",
        "Let the squared loss be defined as,\n",
        "\\begin{equation}\n",
        "\t\\mathcal{L}(x,t) = \\dfrac{1}{2}(y(x) - t)^2, t \\in \\mathbb{R} \n",
        "\\end{equation}\n",
        "We can then the optimal prediction function by minimizing the mean squared error,\n",
        "\\begin{equation}\n",
        "\t\t\\mathcal{E}(w) = \\dfrac{1}{2N}\\sum\\limits_{n=1}^N \\mathcal{L}(x_n, t_n) = \\dfrac{1}{2N}\\sum\\limits_{n=1}^N (w^\\top x_n + b - t_n)^2 \\tag*{(mse)}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNxXD6S6CXGb"
      },
      "source": [
        "In the following code, you will generate a data set $\\mathcal{D}$ consisting of $20$ 1D data points $x_n$ sampled from a uniform distribution along with targets $t_n$ assumed to be given by the formula,\n",
        "\n",
        "$$\n",
        "  t_n = \\sin(\\pi x_n) + 0.3*\\epsilon, \\epsilon \\sim \\mathcal{N}(0,1)\n",
        "$$\n",
        "Let us visualize these data point using the Matplotlib package. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo79VxXU_hTe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "de410d54-4434-40fc-97dd-53a99a16485a"
      },
      "source": [
        "n, d = 20, 1 #n represents data points and d represents the number of dimensions\n",
        "\n",
        "x = random.uniform(key, (n, d), dtype=np.float64,  minval = -5., maxval = 5.)    \n",
        "t = np.sin(np.pi*x) + 0.3*random.normal(key, (n, d))    \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(x, t)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWZKrSwX_zEp"
      },
      "source": [
        "## 2.1 Directly Solving for $w^\\star$ Using the Normal Equation\n",
        "\n",
        "Since our data set is small, therefore we can directly find the optimal predictor using a bit of linear algebra.\n",
        "\n",
        "To account for the bias term, it is common to redefining our data and weight as $x_n := \\begin{bmatrix} 1 & x_n^\\top \\end{bmatrix}$ and $w := \\begin{bmatrix} b & w^\\top \\end{bmatrix}$, \n",
        "$$\n",
        "  y(x_n) = \\textstyle\\sum\\limits_{i = 1}^{d+1} w_i x_{n_i} = w^\\top x_n.\n",
        "$$\n",
        "\n",
        "Then, we can show that our mean squared error can be equivalently written as $$\\mathcal{E}(w) =    \\dfrac{1}{2N} \\|Xw-t\\|^2_2,$$\n",
        "where \\begin{equation}\n",
        "X = \\begin{bmatrix} x_1^\\top \\\\ \\vdots \\\\ x_N^\\top \\end{bmatrix}  \\in \\mathbb{R}^{N \\times (d+1)} \\quad \\text{and} \\quad t = \\begin{bmatrix} t_1\\\\ \\vdots \\\\ t_N \\end{bmatrix} \\in \\mathbb{R}^N\n",
        "\\end{equation}\n",
        "\n",
        "It can be shown that, $$w^\\star = (X^\\top X)^{-1}X^\\top t$$ minimizes the mean-squared error $\\mathcal{E}$. This expression is sometimes referred to as the *normal equation*. \n",
        "\n",
        "**In the space below, build the matrix $X$ and find the optimal solution $w^\\star$. Afterwards, plot the optimal prediction function against your dataset.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKBI6I6FDewW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "52d420ac-848e-4dc6-865b-348adfe7da25"
      },
      "source": [
        "#Bult the X matrix here\n",
        "X = random.uniform(key, (20, 3), dtype=np.float64,  minval = -5., maxval = 5.)  \n",
        "b = np.ones((20,1))\n",
        "\n",
        "X = np.concatenate((b,X),axis=1)\n",
        "tar = np.dot(X,np.array([[1,2,3,4]]).T)+ 0.3*random.normal(key, (20, 1)) \n",
        "#Compute w using the normal equation\n",
        "w = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)),X.T),tar)\n",
        "print(w)\n",
        "#Write your optimal predictor here\n",
        "y = np.dot(X,w) \n",
        "\n",
        "#Plot your prediction against data points\n",
        "index=np.arange(20)\n",
        "plt.plot(index,tar,\"o\",index,y,\"-\")\n",
        "plt.legend(loc='lower left', labels=['target value', 'predicted value'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.8563728]\n",
            " [1.9547423]\n",
            " [2.9778275]\n",
            " [4.0255833]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJb5DlnU_MmR"
      },
      "source": [
        "## 2.2 Find $w^\\star$ Using Gradient Descent\n",
        "\n",
        "Next, you will use gradient descent to find the solution to the linear regression problem. Recall that the update equation for gradient descent (as studied in class) is given by,\n",
        "\n",
        "$$w_{k+1} = w_k - \\alpha \\nabla \\mathcal{E}(w_k)$$\n",
        "\n",
        "$\\alpha$ is our learning rate which is usually a small number, e.g., $0.1$\n",
        "\n",
        "**Implement a jax routine for calculating the gradient of $\\mathcal{E}$, and then run the gradient descent algorithm (say, for 100 iterations) until the optimal weight is found.** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdtO4WdyVyGW"
      },
      "source": [
        "### 2.2.1 **Define the mse function**\n",
        "\n",
        "Hint: your code could be similar to how we built the logistic function at the beginning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prvljG9lV8DI"
      },
      "source": [
        "# Build the mse function here\n",
        "from scipy.special import expit as sigmoid\n",
        "import math\n",
        "def forward(X,tar):\n",
        "  def params(w):\n",
        "    return np.sum((np.dot(X,w)-tar)*(np.dot(X,w)-tar)*0.5)/len(x)\n",
        "  return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MhXNfNGV995"
      },
      "source": [
        "### 2.2.2 **Implement Gradient Descent and Run for (say, 100) Interations**\n",
        "\n",
        "\n",
        "$$w_{k+1} = w_k - \\alpha \\nabla \\mathcal{E}(w_k)$$\n",
        "\n",
        "Hint: The function `grad` takes as arguments:\n",
        "-  `fun`: the numpy **function** for which the computation of the gradients is needed.\n",
        "- `argnums`: the **arguments** of the functions  with respect to which the function will be differentiated\n",
        "\n",
        "Example:\n",
        "```\n",
        "w_gradient, b_gradient = grad(fun = make_mse(x, t), argnums = (0,1))(w,b) \n",
        "```\n",
        "\n",
        "Then use calculated the gradient in your gradient descent update in a FOR loop\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvKaU7Oj9jtm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e85eee3b-f3c2-41c8-cd4f-3d317f8c824e"
      },
      "source": [
        "alpha = 0.1\n",
        "iterations = 100\n",
        "\n",
        "# Update w, b by calculating the gradient and making the update. \n",
        "GDnotjit = grad(forward(X,tar), (0))\n",
        "w_initial = random.uniform(key, (4, 1), dtype=np.float64)\n",
        "print(w_initial)\n",
        "for i in range(iterations):\n",
        "  w_gradient = GDnotjit(w_initial)\n",
        "  w_initial-=alpha*w_gradient\n",
        "  loss=np.sum((np.dot(X,w_initial)-tar)*(np.dot(X,w_initial)-tar)*0.5)/len(x)\n",
        "  if i%10==0:\n",
        "    print(\"w: \",w_initial,loss)\n",
        "w_optimal=w_initial"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.5685525 ]\n",
            " [0.322456  ]\n",
            " [0.8893297 ]\n",
            " [0.47169125]]\n",
            "w:  [[0.9298066]\n",
            " [2.6671119]\n",
            " [2.8518972]\n",
            " [2.8518085]] 5.118276\n",
            "w:  [[0.94474745]\n",
            " [1.9543501 ]\n",
            " [2.9758232 ]\n",
            " [4.0129395 ]] 0.03208787\n",
            "w:  [[0.8909025]\n",
            " [1.9544897]\n",
            " [2.9770794]\n",
            " [4.020877 ]] 0.029052904\n",
            "w:  [[0.8698519]\n",
            " [1.9546436]\n",
            " [2.9775355]\n",
            " [4.023746 ]] 0.028590614\n",
            "w:  [[0.8616343]\n",
            " [1.9547036]\n",
            " [2.9777133]\n",
            " [4.0248656]] 0.02852014\n",
            "w:  [[0.85842645]\n",
            " [1.9547272 ]\n",
            " [2.9777827 ]\n",
            " [4.025303  ]] 0.028509308\n",
            "w:  [[0.85717416]\n",
            " [1.9547361 ]\n",
            " [2.97781   ]\n",
            " [4.0254736 ]] 0.0285077\n",
            "w:  [[0.85668534]\n",
            " [1.9547399 ]\n",
            " [2.9778206 ]\n",
            " [4.0255404 ]] 0.028507465\n",
            "w:  [[0.8564945]\n",
            " [1.9547412]\n",
            " [2.9778247]\n",
            " [4.0255666]] 0.02850744\n",
            "w:  [[0.85642004]\n",
            " [1.9547417 ]\n",
            " [2.977826  ]\n",
            " [4.0255766 ]] 0.028507425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc9nsGzzWtIl"
      },
      "source": [
        "### 2.2.3 **Build your optimal linear prediction function and visualize your fit to the data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_7U1E_YW-8K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "cc5625fd-15ad-49fd-c501-35e008995e67"
      },
      "source": [
        "# Your optimal linear prediction function here\n",
        "Y=np.dot(X,w_optimal)\n",
        "\n",
        "\n",
        "# Visualize your data and your fit\n",
        "plt.plot(index,tar,\"o\",index,Y,\"-\")\n",
        "plt.legend(loc='lower left', labels=['target value', 'predicted value'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLQ6TKZ2Xk8G"
      },
      "source": [
        "### 2.2.4 **(Optional) You can use jit to speed calculation.** \n",
        "\n",
        "**Repeat the gradient descent procedure as before but using jit to speed up the calculation of gradients.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cugsYg-zXv_0"
      },
      "source": [
        "# Build your jitted gradient descent iteration here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQJ1rmRrBdbT"
      },
      "source": [
        "#3.0 Overfitting and Regularization\n",
        "\n",
        "The goal of supervised machine learning is to make accurate predictions on data that are not from the training set, also known as the *test set*. However it is not guaranteed that the more data we train on, the better we will perform on the test set. \n",
        "\n",
        "It could happen that we fit our training set too well, so that a test data that does not look like the training data will cause a large error in our prediction. This is called **overfitting**: the scenario when a predictor that achieves a lower error on the training set results in a large error on the test set.\n",
        "\n",
        "To overcome overfitting, we can use a technique called regularization, which involves adding a function on to our mean squared error. In the following examples, we will step through several examples of regularization techniques. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpN2YCd6rZN-"
      },
      "source": [
        "## 3.1 Building a Dataset\n",
        "\n",
        "The code below creates a data set of  $n = 100$ examples generated from uniformly from $[-5, 5]$, each example is $5$ dimensional. The targets are assigned as follows, $$t  = x^\\top \\overline w + 0.1 * \\epsilon, \\epsilon \\sim \\mathcal{N}(0,1)$$ \n",
        "where $\\overline w$ is a randomly generated vector with normal distribution. \n",
        "\n",
        "\n",
        "**Your first task is to partition the data into 50 test examples, 20 validation examples and 30 test examples.** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3K6GbuJA6g2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b0a1f9-6c4d-42f5-824f-2ff451456413"
      },
      "source": [
        "d = 5                             #dimension of your data\n",
        "n = 100                           #total number of data points\n",
        "n_train = 50                      #total number of training examples\n",
        "n_validate = 20                   #total number of examples for validation \n",
        "n_test = 30                       #total number of examples for testing\n",
        "\n",
        "x_data = random.uniform(key, (n, d), dtype=np.float64,  minval = -5., maxval = 5.)    \n",
        "true_w = random.normal(key, (d,))\n",
        "t_data = x_data.dot(true_w) + 0.1 * random.normal(key, (n,))\n",
        "\n",
        "#Partition into training, validation and test set\n",
        "x_train = x_data[:50]\n",
        "x_val = x_data[50:70]\n",
        "x_test = x_data[70:100]\n",
        "\n",
        "t_train = t_data[:50]\n",
        "t_val = t_data[50:70]\n",
        "t_test = t_data[70:100]\n",
        "print(x_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.4137816   1.430788    4.0541945  -1.2773418  -2.0266628 ]\n",
            " [ 3.9236727  -0.32455206  1.3953028   1.1383653   3.1143055 ]\n",
            " [-2.741723    3.9734507   4.302367    2.4208593  -3.4558427 ]\n",
            " [-1.6669703   1.456387    2.1204448   0.9421692  -1.5382063 ]\n",
            " [-1.3605666  -3.4809303  -4.8397684  -3.6949873  -0.31685734]\n",
            " [-0.6236706  -3.5205686  -3.5384655  -2.0179164  -3.8888943 ]\n",
            " [-3.9208055  -4.327141   -4.348073    3.2808914   0.58691025]\n",
            " [-2.7467191   2.238573    0.48922634  1.2298145   1.9059467 ]\n",
            " [-1.4907634   1.0580468  -1.8860197   0.24154425 -4.4085083 ]\n",
            " [-4.908631    0.98646736 -3.404014    2.3044405   4.449663  ]\n",
            " [ 0.02124786  3.9214525  -0.23204088  0.77283955 -0.27360058]\n",
            " [ 3.4294024  -3.520472   -1.5473902  -3.1539404  -2.9430747 ]\n",
            " [ 2.602532   -2.0462      0.27842522 -0.20933056  0.6970539 ]\n",
            " [-1.9941795   1.4481997  -3.3872437  -3.4926784  -0.1673994 ]\n",
            " [ 0.934535   -3.1480074   3.0547314   4.672592   -3.7734222 ]\n",
            " [-1.6803193   1.8864164  -0.8594656   2.666738   -1.9837177 ]\n",
            " [ 1.6789031   2.4463072   4.1891975  -2.4898386  -2.3841023 ]\n",
            " [-1.6976333   1.6778746  -3.5489428   0.1350236   0.26304626]\n",
            " [-0.20182133 -3.1680954  -3.9286172  -1.446209   -0.6495247 ]\n",
            " [-0.46858883 -4.6104136  -0.41269064 -2.2104108   4.579918  ]\n",
            " [ 4.762985    0.6430483   4.7366524   0.7354927  -0.89155436]\n",
            " [-1.7773223   4.8331976   3.5519466   0.04916906 -4.4863997 ]\n",
            " [-3.0410564  -2.5396383  -4.369603    2.5301766  -0.06345367]\n",
            " [-3.842461    4.5331745  -0.7193303   4.3920345  -4.057467  ]\n",
            " [-2.2009087   3.1856403  -0.7011175  -2.8273332  -0.8765497 ]\n",
            " [-4.1573706   2.2705793  -3.902738   -1.0860944  -2.233286  ]\n",
            " [ 3.426509   -0.40227652 -0.3308525   0.4222703   4.892787  ]\n",
            " [ 1.876595   -3.3042622  -4.8203526  -4.3949485   3.5358953 ]\n",
            " [-0.33437824  4.4986763   1.4823914   4.3998985  -1.2323546 ]\n",
            " [-1.9524288   2.3768291  -4.1829157  -4.09276    -2.2717345 ]\n",
            " [ 1.010561   -3.973645    0.4805746  -2.8441381  -4.32568   ]\n",
            " [-0.29915714  4.020418    3.0655231   3.0905685   2.7407923 ]\n",
            " [-1.8907785  -4.60073    -4.624449   -1.522721    0.814116  ]\n",
            " [-4.5722246   0.10588408 -3.8065612  -1.9777203   3.3488293 ]\n",
            " [ 1.4591513  -0.05182838  0.70620155 -1.7055845  -3.190608  ]\n",
            " [ 0.94321966 -3.0150592  -2.1420228  -4.889865    1.3678274 ]\n",
            " [-1.069746   -3.5648382  -3.2159567  -0.34682655 -1.5038908 ]\n",
            " [-4.9552703   2.4248624  -3.9265049   0.17114258 -4.483076  ]\n",
            " [-1.5320528   4.8487463  -0.77032185  3.0854845  -3.3061087 ]\n",
            " [ 2.2046566   4.480133    1.399734   -2.6068377   1.420176  ]\n",
            " [-2.984271    3.0232544  -0.2312398  -4.8188806   2.7735043 ]\n",
            " [ 1.3595343  -1.6598547   1.6177473  -1.821847    0.24557018]\n",
            " [-3.1293094   4.920389   -1.8583453   4.556488   -2.2946477 ]\n",
            " [-0.8352089  -2.9276073   4.6100855  -3.280989    2.0264888 ]\n",
            " [ 4.994583    0.15920877 -3.2456589   4.527029    1.8413696 ]\n",
            " [ 4.0587034  -0.6455183   4.6347427  -1.0426724   1.847683  ]\n",
            " [-0.7507348   0.08593464 -1.4720106   4.2659435  -3.8372111 ]\n",
            " [ 0.46246147  0.21019697  0.9429836   1.6662827   4.453747  ]\n",
            " [-4.1438046   1.9436002   0.40996933  3.979992   -2.8024554 ]\n",
            " [ 3.6562786   4.5921946   1.2072563   2.1852245   0.33908272]\n",
            " [-3.1122053  -4.2194486   2.2664843  -4.9388576  -4.4812274 ]\n",
            " [-0.87506866  4.8004627  -0.45037985 -4.1433907  -1.7593503 ]\n",
            " [-1.0760772   4.0093975   4.7806396  -3.5845637   1.4595103 ]\n",
            " [-3.5787952  -2.005124    0.09581184  1.3039246  -4.2871485 ]\n",
            " [-2.9399323  -4.9265265  -2.959417   -1.4929998  -4.9886284 ]\n",
            " [-1.6952562  -3.5139382   2.6537523  -0.34901237 -4.5598364 ]\n",
            " [-2.8827918   3.5528803  -3.868301    1.5982761  -1.4756691 ]\n",
            " [-3.1926048   0.7807293  -1.984061    0.18369198  2.6347733 ]\n",
            " [ 0.6668806   4.185034    0.9008064   0.4956665   1.7274666 ]\n",
            " [ 4.8055277  -1.9746053   4.755269   -4.7806797   2.066926  ]\n",
            " [ 1.0238304   3.749712    2.4616194   1.0014858  -3.283118  ]\n",
            " [ 4.681736   -3.033501   -0.96269035 -0.04789734 -2.4832332 ]\n",
            " [-1.9692135   3.6185703  -1.1163533   0.18836594 -2.356137  ]\n",
            " [-2.5009382  -3.2418048  -0.87965107 -2.736212   -4.5253096 ]\n",
            " [-4.766988   -1.3577521  -1.6693294  -0.22660637  0.8170452 ]\n",
            " [-0.82331085  1.8665943  -0.26395226  0.04995251  4.865679  ]\n",
            " [ 3.6086826   3.1318645  -1.8474364  -4.1789436  -0.3326645 ]\n",
            " [ 1.6548872  -2.7106953  -4.977003    3.430893    2.930591  ]\n",
            " [-1.16997     0.63158417 -1.9682288   4.0509796  -2.5931072 ]\n",
            " [-2.6216257  -3.093747    4.668602   -0.46237087 -2.3085034 ]\n",
            " [-1.661346    2.1552029  -0.8494806  -1.8656003   3.022356  ]\n",
            " [-0.69103813  0.8347044  -1.536659    3.7136507  -2.3189354 ]\n",
            " [-4.940442    1.1244764  -3.8237357   3.1770344  -2.6271892 ]\n",
            " [-2.4690056  -2.310977    3.9212837   3.9103947  -0.4360962 ]\n",
            " [-2.0502913   1.6764631   2.1695652   1.3226652  -4.0424786 ]\n",
            " [-2.354921   -4.0099406   2.3889017  -1.9719124  -0.19637108]\n",
            " [ 1.9787359   1.0847149   2.2714639   2.7787662   1.7010746 ]\n",
            " [-1.2127197   3.7736902  -1.5251887  -2.444967    1.9880629 ]\n",
            " [ 1.952312   -3.3114207  -0.47912598  2.4963274   2.9427109 ]\n",
            " [ 1.4335608   4.92118     4.5374956   0.82632065 -1.7579222 ]\n",
            " [ 3.0879602  -4.6982455   3.7853317   1.183485    3.571333  ]\n",
            " [-1.2676096   3.5298548   4.068228    1.5481949   0.6424942 ]\n",
            " [ 3.8796082   2.3851786  -1.8071854  -2.9169106  -2.094053  ]\n",
            " [ 1.3773308   2.2653246   3.1064491  -3.341918   -2.2292936 ]\n",
            " [-1.800977   -1.4505947  -2.570026    1.825006    2.8754973 ]\n",
            " [-1.4134347   2.627561    2.7710114   4.8524704  -4.6722865 ]\n",
            " [-1.5918374   4.2373905  -4.884285    1.7538071  -2.0812392 ]\n",
            " [ 3.461114    1.0364637  -0.0090456   1.4762964   0.30557394]\n",
            " [-2.4229956  -1.9789982  -1.6497195   4.888151   -4.115393  ]\n",
            " [-2.3427105   2.060443    0.69908714  2.9317532   4.09194   ]\n",
            " [-4.2524624  -0.12559414  2.2042952  -0.17577076 -4.9666476 ]\n",
            " [ 1.671145   -0.43465233  4.691619   -3.075608    1.9116569 ]\n",
            " [-0.28837204  2.4378386   0.7907343   4.513936   -3.8133907 ]\n",
            " [ 2.5042143   2.6273766  -0.06359339  3.2931662   0.1842537 ]\n",
            " [-2.3094952  -2.479843   -3.4309316   3.166151    3.8979416 ]\n",
            " [ 1.596129    3.8872395  -2.4151516  -3.9591122  -1.6849446 ]\n",
            " [ 2.8624487  -4.5166945  -2.4588478  -3.2418156   4.4525623 ]\n",
            " [-4.456756    2.5596762  -3.7011278  -1.8942595  -3.9910698 ]\n",
            " [-2.154379   -2.9366815  -2.6815653   1.8865466   2.9898906 ]\n",
            " [ 0.763937    0.07937908  1.5337849   1.3712225   2.6948538 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo_PLbWzdCBX"
      },
      "source": [
        "## 3.2 Solve for $w^\\star$ Using Gradient Descent\n",
        "\n",
        "Next, using the same linear regresion code that you have built above, using gradient descent, compute the optimal predictor $y = x^\\top w^\\star + b^\\star$ corresponding to the mean-squared error \n",
        "$$\\mathcal{E}(w) = \\dfrac{1}{2N}\\sum\\limits_{n=1}^N (w^\\top x_n + b - t_n)^2$$ \n",
        "\n",
        "Afterwards, find the error between the predicted targets on the test set versus the true test targets using the formula,\n",
        "$$\\|y_\\text{test}- t_\\text{test}\\|_2$$\n",
        "where $y_\\text{test}$ is your prediction on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "239r52sHRgsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e431d68-09bb-49ce-8655-6689808d257d"
      },
      "source": [
        "#Define your mse function here \n",
        "def mse(x,t):\n",
        "  def error(w):\n",
        "    return np.sum((np.dot(x,w)-t)*(np.dot(x,w)-t)*0.5)/len(x)\n",
        "  return error\n",
        "#Perform gradient descent\n",
        "learningrate = 0.01\n",
        "iterations = 100\n",
        " \n",
        "GD = grad(mse(x_train,t_train), (0))\n",
        "w_initial = random.uniform(key, (d,), dtype=np.float64)\n",
        "print(w_initial)\n",
        "for i in range(iterations):\n",
        "  w_gradient = GD(w_initial)\n",
        "  w_initial-=learningrate*w_gradient\n",
        "  aggerr=np.sum((np.dot(x_train,w_initial)-t_train)*(np.dot(x_train,w_initial)-t_train)*0.5)/len(x_train)\n",
        "  if i%10==0:\n",
        "    print(\"w: \",w_initial,aggerr)\n",
        "\n",
        "loss=np.sum((np.dot(x_val,w_initial)-t_val)*(np.dot(x_val,w_initial)-t_val)*0.5)/len(x_val)\n",
        "if loss>aggerr:\n",
        "  print(\"validation fails\",loss)\n",
        "else:\n",
        "  print(\"validation completed\")\n",
        "\n",
        "w_optimal=w_initial\n",
        "\n",
        "#Compute and print your test error\n",
        "y_test=x_test.dot(w_optimal)\n",
        "testerr=math.sqrt(np.sum((y_test-t_test)*(y_test-t_test)))\n",
        "print(testerr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.83079386 0.1660409  0.45611215 0.6428884  0.56865394]\n",
            "w:  [0.8370034  0.04520406 0.37929025 0.58540785 0.567572  ] 7.6980267\n",
            "w:  [ 0.8798487  -0.6006399  -0.00203413  0.33893847  0.46062616] 0.86957777\n",
            "w:  [ 0.90798855 -0.8076084  -0.09354074  0.31384078  0.3444836 ] 0.18406372\n",
            "w:  [ 0.92749095 -0.88839495 -0.11519028  0.3261901   0.27063826] 0.052927516\n",
            "w:  [ 0.94010526 -0.9254346  -0.12025023  0.33969313  0.2289996 ] 0.01812475\n",
            "w:  [ 0.94784355 -0.9441278  -0.12151283  0.34910834  0.20641069] 0.008023058\n",
            "w:  [ 0.9524578  -0.9540061  -0.12195915  0.35500154  0.19430926] 0.005020983\n",
            "w:  [ 0.9551798  -0.959324   -0.12223489  0.35855085  0.18784556] 0.0041198614\n",
            "w:  [ 0.956787   -0.9622016  -0.12245855  0.36065286  0.1843903 ] 0.0038472917\n",
            "w:  [ 0.9577443  -0.96375597 -0.12264467  0.36188662  0.1825386 ] 0.0037641686\n",
            "validation fails 0.004698316\n",
            "0.6157995383221567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl1r0yIScAo7"
      },
      "source": [
        "## 3.3 $l_2$ regularization\n",
        "\n",
        "The first regularization technique we will introduce is called $l_2$ regularization. \n",
        "\n",
        "We define our mean-squared loss function as\n",
        "$$\\mathcal{E}_{l_2}(w) = \\dfrac{1}{2N}\\sum\\limits_{n=1}^N (w^\\top x_n + b - t_n)^2 + \\dfrac{\\lambda_2}{2}\\|w\\|^2_2$$\n",
        "where $\\lambda_2$ is a small positive constant we refer to as the regularization constant, e.g., $r = 0.01$. \n",
        "\n",
        "This optimization problem is called *ridge regression*. \n",
        "\n",
        "Make slight changes to your previous code to account for the $l_2$ norm, and using gradient descent, compute the optimal predictor $y = x^\\top w^\\star + b^\\star$ corresponding to this regularized mean-squared error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-uUXTKk2WqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87882a30-0490-4d0a-c7ba-56bcae593d03"
      },
      "source": [
        "#Define your mse function here \n",
        "def mse(x,t):\n",
        "  def error(w,r):\n",
        "    return np.sum((np.dot(x,w)-t)*(np.dot(x,w)-t)*0.5)/len(x)+r*0.5*np.sum(w*w)\n",
        "  return error \n",
        "\n",
        "#Perform gradient descent\n",
        "learningrate = 0.01\n",
        "iterations = 100\n",
        "r=0.01 \n",
        "GD = grad(mse(x_train,t_train), (0))\n",
        "w_initial = random.uniform(key, (d,), dtype=np.float64)\n",
        "print(w_initial)\n",
        "for i in range(iterations):\n",
        "  w_gradient = GD(w_initial,r)\n",
        "  w_initial-=learningrate*w_gradient\n",
        "  aggerr=np.sum((np.dot(x_train,w_initial)-t_train)*(np.dot(x_train,w_initial)-t_train)*0.5)/len(x_train)+r*0.5*np.sum(w_initial*w_initial)\n",
        "  if i%10==0:\n",
        "    print(\"w: \",w_initial,aggerr)\n",
        "\n",
        "loss=np.sum((np.dot(x_val,w_initial)-t_val)*(np.dot(x_val,w_initial)-t_val)*0.5)/len(x_val)+r*0.5*np.sum(w_initial*w_initial)\n",
        "if loss>aggerr:\n",
        "  print(\"validation fails\",loss,loss-aggerr)\n",
        "else:\n",
        "  print(\"validation completed\")\n",
        "\n",
        "w_optimal=w_initial\n",
        "\n",
        "#Compute and print your test error\n",
        "y_test=x_test.dot(w_optimal)\n",
        "testerr=math.sqrt(np.sum((y_test-t_test)*(y_test-t_test)))\n",
        "print(testerr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.83079386 0.1660409  0.45611215 0.6428884  0.56865394]\n",
            "w:  [0.8369203  0.04518746 0.37924463 0.58534354 0.56751513] 7.704812\n",
            "w:  [ 0.8792205  -0.6004217  -0.00204661  0.33854747  0.46025792] 0.8769959\n",
            "w:  [ 0.9070743  -0.80709803 -0.09335087  0.31326294  0.34413075] 0.19315135\n",
            "w:  [ 0.9263899  -0.8876985  -0.11485956  0.32545838  0.27040595] 0.06266121\n",
            "w:  [ 0.9388703  -0.9246258  -0.11983083  0.3388452   0.2288896 ] 0.028115332\n",
            "w:  [ 0.94651115 -0.94325143 -0.12103494  0.34817934  0.2063957 ] 0.01811276\n",
            "w:  [ 0.951055   -0.95309    -0.12144034  0.35401845  0.19436076] 0.015147542\n",
            "w:  [ 0.9537263  -0.9583855  -0.12168634  0.35753253  0.18794124] 0.014259808\n",
            "w:  [ 0.95529735 -0.9612515  -0.12188791  0.359612    0.18451455] 0.013992036\n",
            "w:  [ 0.9562286  -0.9628006  -0.12205748  0.36083144  0.18268101] 0.01391064\n",
            "validation fails 0.014787367 0.0009003151\n",
            "0.617477831577725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qu-GjdOuir0"
      },
      "source": [
        "### 3.3.1 Hyperparameter Tuning Using Validation Set\n",
        "\n",
        "Next, you are going to tune the regularization parameter $\\lambda_2$ using your validation set. \n",
        "\n",
        "Train several different models for $\\lambda_2 \\in \\{0.1, 0.01, 0.001\\}$. Then compute the error between your prediction on the validation set and the corresponding targets using the formula,\n",
        "$$\\|y_\\text{val}- t_\\text{val}\\|_2$$\n",
        "\n",
        "Pick the best performing model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5_AF5SjdJnK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8abc752f-3b1a-4bc2-f5bb-5ff98b98938a"
      },
      "source": [
        "#Your code Here\n",
        "r=np.array([0.1,0.01,0.001])\n",
        "w_initial = random.uniform(key, (d,), dtype=np.float64)\n",
        "print(w_initial)\n",
        "for k in range(len(r)):\n",
        "  w_exe=w_initial\n",
        "  def mse(x,t):\n",
        "    def error(w,r):\n",
        "      return np.sum((np.dot(x,w)-t)*(np.dot(x,w)-t)*0.5)/len(x)+r[k]*0.5*np.sum(w*w)\n",
        "    return error \n",
        "  GD = grad(mse(x_train,t_train), (0))\n",
        "  for i in range(iterations):\n",
        "    w_gradient = GD(w_exe,r)\n",
        "    w_exe-=learningrate*w_gradient\n",
        "    aggerr=np.sum((np.dot(x_train,w_exe)-t_train)*(np.dot(x_train,w_exe)-t_train)*0.5)/len(x_train)+r[k]*0.5*np.sum(w_exe*w_exe)\n",
        "    if i%10==0:\n",
        "      print(\"iteration: \",i,\"loss: \",aggerr)\n",
        "  print(\"learning done\")\n",
        "  y_val=x_val.dot(w_exe)\n",
        "  valerr=math.sqrt(np.sum((y_val-t_val)*(y_val-t_val)))\n",
        "  print(\"validaiton error: \",valerr)\n",
        "#pick the best performing model\n",
        "r=0.01\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.83079386 0.1660409  0.45611215 0.6428884  0.56865394]\n",
            "iteration:  0 loss:  7.765745\n",
            "iteration:  10 loss:  0.9431494\n",
            "iteration:  20 loss:  0.27403587\n",
            "iteration:  30 loss:  0.14918116\n",
            "iteration:  40 loss:  0.1168496\n",
            "iteration:  50 loss:  0.10769016\n",
            "iteration:  60 loss:  0.10503388\n",
            "iteration:  70 loss:  0.10425641\n",
            "iteration:  80 loss:  0.10402739\n",
            "iteration:  90 loss:  0.10395959\n",
            "learning done\n",
            "validaiton error:  0.48931104785383583\n",
            "iteration:  0 loss:  7.704812\n",
            "iteration:  10 loss:  0.8769959\n",
            "iteration:  20 loss:  0.19315135\n",
            "iteration:  30 loss:  0.06266121\n",
            "iteration:  40 loss:  0.028115332\n",
            "iteration:  50 loss:  0.01811276\n",
            "iteration:  60 loss:  0.015147542\n",
            "iteration:  70 loss:  0.014259808\n",
            "iteration:  80 loss:  0.013992036\n",
            "iteration:  90 loss:  0.01391064\n",
            "learning done\n",
            "validaiton error:  0.4324355766792823\n",
            "iteration:  0 loss:  7.698706\n",
            "iteration:  10 loss:  0.8703203\n",
            "iteration:  20 loss:  0.18497333\n",
            "iteration:  30 loss:  0.05390199\n",
            "iteration:  40 loss:  0.019125013\n",
            "iteration:  50 loss:  0.009033271\n",
            "iteration:  60 loss:  0.006034908\n",
            "iteration:  70 loss:  0.0051351422\n",
            "iteration:  80 loss:  0.004863055\n",
            "iteration:  90 loss:  0.0047801062\n",
            "learning done\n",
            "validaiton error:  0.4333322440952892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sydfmSVuvRU"
      },
      "source": [
        "### 3.3.2 Compute Test Error\n",
        "\n",
        "Finally, using the best performing model, find the error between the predicted targets on the test set versus the true test targets using the formula,\n",
        "$$\\|y_\\text{test}- t_\\text{test}\\|_2$$\n",
        "where $y_\\text{test}$ is your prediction on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2rMuty9dIU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba5a4fa-b548-4caf-8fc8-e014b692809f"
      },
      "source": [
        "# Your code Here\n",
        "#Define your mse function here \n",
        "def mse(x,t):\n",
        "  def error(w,r):\n",
        "    return np.sum((np.dot(x,w)-t)*(np.dot(x,w)-t)*0.5)/len(x)+r*0.5*np.sum(w*w)\n",
        "  return error \n",
        "\n",
        "#Perform gradient descent\n",
        "learningrate = 0.01\n",
        "iterations = 100\n",
        "r=0.001 \n",
        "GD = grad(mse(x_train,t_train), (0))\n",
        "w_initial = random.uniform(key, (d,), dtype=np.float64)\n",
        "print(w_initial)\n",
        "for i in range(iterations):\n",
        "  w_gradient = GD(w_initial,r)\n",
        "  w_initial-=learningrate*w_gradient\n",
        "  aggerr=np.sum((np.dot(x_train,w_initial)-t_train)*(np.dot(x_train,w_initial)-t_train)*0.5)/len(x_train)+r*0.5*np.sum(w_initial*w_initial)\n",
        "  if i%10==0:\n",
        "    print(\"iteration: \",i,\"loss: \",aggerr)\n",
        "w_optimal=w_initial\n",
        "\n",
        "#Compute and print your test error\n",
        "y_test=x_test.dot(w_optimal)\n",
        "testerr=math.sqrt(np.sum((y_test-t_test)*(y_test-t_test)))\n",
        "print(testerr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.83079386 0.1660409  0.45611215 0.6428884  0.56865394]\n",
            "iteration:  0 loss:  7.698706\n",
            "iteration:  10 loss:  0.8703203\n",
            "iteration:  20 loss:  0.18497333\n",
            "iteration:  30 loss:  0.05390199\n",
            "iteration:  40 loss:  0.019125013\n",
            "iteration:  50 loss:  0.009033271\n",
            "iteration:  60 loss:  0.006034908\n",
            "iteration:  70 loss:  0.0051351422\n",
            "iteration:  80 loss:  0.004863055\n",
            "iteration:  90 loss:  0.0047801062\n",
            "0.6159172264954724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A18w7Gr9cJ0m"
      },
      "source": [
        "## 3.4 $l_1$ Regularization \n",
        "\n",
        "The next regularization technique we will introduce is called $l_1$ regularization. \n",
        "\n",
        "We define our mean-squared loss function as \n",
        "$$\\mathcal{E}_{l_1}(w) = \\dfrac{1}{2N}\\sum\\limits_{n=1}^N (w^\\top x_n + b - t_n)^2 + \\dfrac{\\lambda_1}{2}\\|w\\|_1$$\n",
        "where $\\lambda_1$ is again our regularization constant.\n",
        "\n",
        "This optimization problem is called the *LASSO*. \n",
        "\n",
        "**Repeat the previous experiment by training several different models and choosing the best one based on its performance on the validation set.** \n",
        "\n",
        "**Finally, pick the best performing model and use it to find the error between the predicted targets on the test set versus the true test targets.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncDETNSk_--d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df9ea67-3dce-44d6-8383-f4a943e7df51"
      },
      "source": [
        "# Your code here\n",
        "r=np.array([0.1,0.01,0.001])\n",
        "w_initial = random.uniform(key, (d,), dtype=np.float64)\n",
        "print(w_initial)\n",
        "for k in range(len(r)):\n",
        "  w_exe=w_initial\n",
        "  def mse(x,t):\n",
        "    def error(w,r):\n",
        "      return np.sum((np.dot(x,w)-t)*(np.dot(x,w)-t)*0.5)/len(x)+r[k]*0.5*np.linalg.norm(w,1)\n",
        "    return error \n",
        "  GD = grad(mse(x_train,t_train), (0))\n",
        "  for i in range(iterations):\n",
        "    w_gradient = GD(w_exe,r)\n",
        "    w_exe-=learningrate*w_gradient\n",
        "    aggerr=np.sum((np.dot(x_train,w_exe)-t_train)*(np.dot(x_train,w_exe)-t_train)*0.5)/len(x_train)+r[k]*0.5*np.linalg.norm(w_exe,1)\n",
        "    if i%10==0:\n",
        "      print(\"iteration: \",i,\"loss: \",aggerr)\n",
        "  print(\"learning done\")\n",
        "  y_val=x_val.dot(w_exe)\n",
        "  valerr=math.sqrt(np.sum((y_val-t_val)*(y_val-t_val)))\n",
        "  print(\"validaiton error: \",valerr)\n",
        "#pick the best performing model\n",
        "r=0.001\n",
        "#Compute and print your test error\n",
        "w_exe=w_initial\n",
        "def mse(x,t):\n",
        "    def error(w,r):\n",
        "      return np.sum((np.dot(x,w)-t)*(np.dot(x,w)-t)*0.5)/len(x)+r*0.5*np.linalg.norm(w,1)\n",
        "    return error \n",
        "GD = grad(mse(x_train,t_train), (0))\n",
        "for i in range(iterations):\n",
        "  w_gradient = GD(w_exe,r)\n",
        "  w_exe-=learningrate*w_gradient\n",
        "  aggerr=np.sum((np.dot(x_train,w_exe)-t_train)*(np.dot(x_train,w_exe)-t_train)*0.5)/len(x_train)+r*0.5*np.linalg.norm(w_exe,1)\n",
        "  if i%10==0:\n",
        "    print(\"iteration: \",i,\"loss: \",aggerr)\n",
        "print(\"learning done\")\n",
        "y_val=x_val.dot(w_exe)\n",
        "valerr=math.sqrt(np.sum((y_val-t_val)*(y_val-t_val)))\n",
        "print(\"validaiton error: \",valerr)\n",
        "w_optimal=w_exe\n",
        "y_test=x_test.dot(w_optimal)\n",
        "testerr=math.sqrt(np.sum((y_test-t_test)*(y_test-t_test)))\n",
        "print(testerr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.83079386 0.1660409  0.45611215 0.6428884  0.56865394]\n",
            "iteration:  0 loss:  7.8076334\n",
            "iteration:  10 loss:  0.98115426\n",
            "iteration:  20 loss:  0.31005502\n",
            "iteration:  30 loss:  0.1807274\n",
            "iteration:  40 loss:  0.14640722\n",
            "iteration:  50 loss:  0.13647585\n",
            "iteration:  60 loss:  0.1335365\n",
            "iteration:  70 loss:  0.13265863\n",
            "iteration:  80 loss:  0.13239479\n",
            "iteration:  90 loss:  0.13231508\n",
            "learning done\n",
            "validaiton error:  0.5133296566321661\n",
            "iteration:  0 loss:  7.7089987\n",
            "iteration:  10 loss:  0.88076234\n",
            "iteration:  20 loss:  0.19673368\n",
            "iteration:  30 loss:  0.06579132\n",
            "iteration:  40 loss:  0.031042036\n",
            "iteration:  50 loss:  0.020959847\n",
            "iteration:  60 loss:  0.017965302\n",
            "iteration:  70 loss:  0.017067164\n",
            "iteration:  80 loss:  0.016795821\n",
            "iteration:  90 loss:  0.016713232\n",
            "learning done\n",
            "validaiton error:  0.4342800242377959\n",
            "iteration:  0 loss:  7.6991234\n",
            "iteration:  10 loss:  0.8706964\n",
            "iteration:  20 loss:  0.18533157\n",
            "iteration:  30 loss:  0.054214805\n",
            "iteration:  40 loss:  0.01941742\n",
            "iteration:  50 loss:  0.009317661\n",
            "iteration:  60 loss:  0.0063163484\n",
            "iteration:  70 loss:  0.0054155267\n",
            "iteration:  80 loss:  0.005143082\n",
            "iteration:  90 loss:  0.005060016\n",
            "learning done\n",
            "validaiton error:  0.43351005914978313\n",
            "iteration:  0 loss:  7.6991234\n",
            "iteration:  10 loss:  0.8706964\n",
            "iteration:  20 loss:  0.18533157\n",
            "iteration:  30 loss:  0.054214805\n",
            "iteration:  40 loss:  0.01941742\n",
            "iteration:  50 loss:  0.009317661\n",
            "iteration:  60 loss:  0.0063163484\n",
            "iteration:  70 loss:  0.0054155267\n",
            "iteration:  80 loss:  0.005143082\n",
            "iteration:  90 loss:  0.005060016\n",
            "learning done\n",
            "validaiton error:  0.43351005914978313\n",
            "0.6155285813011863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr1f2ax2cQ6V"
      },
      "source": [
        "## 3.5 Approximate $l_1$ Regularization\n",
        "\n",
        "A technicality is that the  $l_1$ norm is not differentiable, so JAX was actually avoiding the non-differentiable points.\n",
        "\n",
        "To address the non-differentiability of $l_1$ norm, Schmidt *et al.* [1] proposed a differentiable approximation to the $l_1$ norm.\n",
        "\n",
        "In this case, we define our mean-squared loss function as \n",
        "$$\\mathcal{E}_{\\widetilde{l_1}}(w) = \\dfrac{1}{2N}\\sum\\limits_{n=1}^N (w^\\top x_n + b - t_n)^2 + \\sum\\limits_{i = 1}^d \\dfrac{\\lambda}{a} (\\log(1+\\exp(a w_i)) + \\log(1-\\exp(-a w_i)))$$\n",
        "where both $a$ and $\\lambda$ are regularization constants.\n",
        "\n",
        "\n",
        "**Repeat the previous experiments by training several different models and choosing the best one based on its performance on the validation set.** \n",
        "\n",
        "(For convenience, you can fix $a$, say $a = 0.1$. Alternatively you can tune both $a$ and $\\lambda$)\n",
        "\n",
        "**Finally, pick the best performing model and use it to find the error between the predicted targets on the test set versus the true test targets.**\n",
        "\n",
        "[1] *Mark Schmidt, Glenn Fung, and Rmer Rosales. Fast\n",
        "optimization methods for l1 regularization: A comparative study and two new approaches. In European Conference on Machine Learning, pages 286â297.\n",
        "Springer, 2007.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNpXfqcJVhQd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53442a86-717d-4391-dbd2-361700d217eb"
      },
      "source": [
        "# Your code here\n",
        "\n",
        "r=np.array([0.1,0.01,0.001])\n",
        "w_initial = random.uniform(key, (d,), dtype=np.float64)\n",
        "learningrate=0.01\n",
        "\n",
        "for k in range(len(r)):\n",
        "  w_exe=w_initial\n",
        "  def mse(x,t):\n",
        "    def error(w,r):\n",
        "      return np.sum((np.dot(x,w)-t)*(np.dot(x,w)-t)*0.5)/len(x)+(r[k]/0.1)*np.sum(np.log(1+np.exp(0.1*w))+np.log(1+np.exp(-0.1*w)))\n",
        "    return error \n",
        "  GD = grad(mse(x_train,t_train), (0))\n",
        "  for i in range(iterations):\n",
        "    w_gradient = GD(w_exe,r)\n",
        "    w_exe-=learningrate*w_gradient\n",
        "    aggerr=np.sum((np.dot(x_train,w_exe)-t_train)*(np.dot(x_train,w_exe)-t_train)*0.2)/len(x_train)+(r[k]/0.1)*np.sum(np.log(1+np.exp(0.1*w_exe))+np.log(1+np.exp(-0.1*w_exe)))\n",
        "    if i%10==0:\n",
        "      print(\"iteration: \",i,\"loss: \",aggerr,\"w\",w_exe)\n",
        "  print(\"learning done\")\n",
        "  y_val=x_val.dot(w_exe)\n",
        "  valerr=math.sqrt(np.sum((y_val-t_val)*(y_val-t_val)))\n",
        "  print(\"validaiton error: \",valerr)\n",
        "#pick the best performing model\n",
        "r=0.01 \n",
        "w_exe=w_initial\n",
        "def mse(x,t):\n",
        "    def error(w,r):\n",
        "      return np.sum((np.dot(x,w)-t)*(np.dot(x,w)-t)*0.5)/len(x)+(r/0.1)*np.sum(np.log(1+np.exp(0.1*w))+np.log(1+np.exp(-0.1*w)))\n",
        "    return error \n",
        "GD = grad(mse(x_train,t_train), (0))\n",
        "for i in range(iterations):\n",
        "  w_gradient = GD(w_exe,r)\n",
        "  w_exe-=learningrate*w_gradient\n",
        "  aggerr=np.sum((np.dot(x_train,w_exe)-t_train)*(np.dot(x_train,w_exe)-t_train)*0.2)/len(x_train)+(r/0.1)*np.sum(np.log(1+np.exp(0.1*w_exe))+np.log(1+np.exp(-0.1*w_exe)))\n",
        "  if i%10==0:\n",
        "    print(\"iteration: \",i,\"loss: \",aggerr)\n",
        "print(\"learning done\")\n",
        "y_val=x_val.dot(w_exe)\n",
        "valerr=math.sqrt(np.sum((y_val-t_val)*(y_val-t_val)))\n",
        "print(\"validaiton error: \",valerr)\n",
        "w_optimal=w_exe\n",
        "y_test=x_test.dot(w_optimal)\n",
        "testerr=math.sqrt(np.sum((y_test-t_test)*(y_test-t_test)))\n",
        "print(testerr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration:  0 loss:  10.014305 w [0.83696187 0.04519575 0.37926745 0.5853757  0.5675436 ]\n",
            "iteration:  10 loss:  7.282977 w [ 0.87953466 -0.60053086 -0.00204039  0.33874297  0.46044198]\n",
            "iteration:  20 loss:  7.009466 w [ 0.9075315  -0.8073534  -0.09344581  0.31355184  0.3443071 ]\n",
            "iteration:  30 loss:  6.95735 w [ 0.9269405  -0.88804686 -0.11502488  0.32582417  0.27052194]\n",
            "iteration:  40 loss:  6.9436 w [ 0.9394878  -0.92503035 -0.12004042  0.339269    0.22894448]\n",
            "iteration:  50 loss:  6.939649 w [ 0.94717723 -0.9436899  -0.1212737   0.3486437   0.20640309]\n",
            "iteration:  60 loss:  6.938495 w [ 0.95175624 -0.9535482  -0.1216995   0.35450977  0.19433494]\n",
            "iteration:  70 loss:  6.9381604 w [ 0.95445293 -0.9588549  -0.12196036  0.35804144  0.18789338]\n",
            "iteration:  80 loss:  6.9380655 w [ 0.956042   -0.9617267  -0.12217296  0.36013216  0.18445243]\n",
            "iteration:  90 loss:  6.9380407 w [ 0.9569863  -0.9632784  -0.12235078  0.3613587   0.18260981]\n",
            "learning done\n",
            "validaiton error:  0.43277300230958643\n",
            "iteration:  0 loss:  3.7727203 w [0.83699924 0.04520323 0.37928796 0.58540463 0.5675692 ]\n",
            "iteration:  10 loss:  1.041346 w [ 0.87981725 -0.600629   -0.00203477  0.3389189   0.46060777]\n",
            "iteration:  20 loss:  0.7672099 w [ 0.9079428  -0.80758286 -0.09353124  0.31381184  0.34446597]\n",
            "iteration:  30 loss:  0.7147893 w [ 0.9274358  -0.88836014 -0.11517373  0.32615346  0.27062663]\n",
            "iteration:  40 loss:  0.7008853 w [ 0.9400434  -0.9253941  -0.12022921  0.33965066  0.22899409]\n",
            "iteration:  50 loss:  0.6968536 w [ 0.9477768  -0.9440839  -0.12148888  0.3490618   0.20640995]\n",
            "iteration:  60 loss:  0.6956575 w [ 0.9523876  -0.9539602  -0.12193314  0.35495228  0.19431186]\n",
            "iteration:  70 loss:  0.6952997 w [ 0.95510703 -0.959277   -0.12220741  0.35849988  0.18785036]\n",
            "iteration:  80 loss:  0.695192 w [ 0.95671237 -0.96215403 -0.12242994  0.36060074  0.18439656]\n",
            "iteration:  90 loss:  0.6951595 w [ 0.95766836 -0.96370816 -0.12261523  0.36183378  0.18254577]\n",
            "learning done\n",
            "validaiton error:  0.43341995742820877\n",
            "iteration:  0 loss:  3.1485612 w [0.837003   0.04520398 0.37929    0.5854075  0.56757176]\n",
            "iteration:  10 loss:  0.4171826 w [ 0.87984556 -0.6006388  -0.00203419  0.33893648  0.4606243 ]\n",
            "iteration:  20 loss:  0.14298397 w [ 0.9079839  -0.80760586 -0.09353975  0.31383786  0.3444819 ]\n",
            "iteration:  30 loss:  0.09053287 w [ 0.92748535 -0.8883914  -0.1151886   0.32618645  0.27063712]\n",
            "iteration:  40 loss:  0.07661346 w [ 0.940099   -0.9254305  -0.12024811  0.33968887  0.22899908]\n",
            "iteration:  50 loss:  0.07257368 w [ 0.9478368  -0.9441234  -0.12151042  0.3491037   0.20641065]\n",
            "iteration:  60 loss:  0.07137332 w [ 0.95245075 -0.95400155 -0.12195652  0.35499662  0.19430955]\n",
            "iteration:  70 loss:  0.07101313 w [ 0.9551725  -0.95931923 -0.12223212  0.35854584  0.18784605]\n",
            "iteration:  80 loss:  0.07090425 w [ 0.95677954 -0.9621968  -0.12245568  0.36064768  0.18439098]\n",
            "iteration:  90 loss:  0.07087107 w [ 0.95773673 -0.96375114 -0.12264173  0.36188135  0.18253937]\n",
            "learning done\n",
            "validaiton error:  0.4335030813184651\n",
            "iteration:  0 loss:  3.7727203\n",
            "iteration:  10 loss:  1.0413461\n",
            "iteration:  20 loss:  0.7672099\n",
            "iteration:  30 loss:  0.71478933\n",
            "iteration:  40 loss:  0.70088536\n",
            "iteration:  50 loss:  0.69685364\n",
            "iteration:  60 loss:  0.69565755\n",
            "iteration:  70 loss:  0.69529974\n",
            "iteration:  80 loss:  0.69519204\n",
            "iteration:  90 loss:  0.69515955\n",
            "learning done\n",
            "validaiton error:  0.43341995742820877\n",
            "0.6158570060602153\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyCNgrkzcW3r"
      },
      "source": [
        "## 3.6 Elastic Net\n",
        "\n",
        "Finally, we can define our mean-squared loss function as a combination of $l_1$ and $l_2$ norm, \n",
        "$$\\mathcal{E}_{l_1+l_2}(w) = \\dfrac{1}{2N}\\sum\\limits_{n=1}^N (w^\\top x_n + b - t_n)^2 + \\frac{r_1}{2}\\|w\\|_1 + \\dfrac{r_2}{2}\\|w\\|_2^2$$\n",
        "\n",
        "This regularizer is called the elastic net. \n",
        "\n",
        "**Repeat the previous experiments by training several different models and choosing the best one based on its performance on the validation set.** \n",
        "\n",
        "**Finally, pick the best performing model and use it to find the error between the predicted targets on the test set versus the true test targets.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZOjqRiPLtmr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c918b344-42de-4634-d011-e1420c7c9dc7"
      },
      "source": [
        "# Your code here: \n",
        "r=np.array([0.1,0.01,0.001])\n",
        "w_initial = random.uniform(key, (d,), dtype=np.float64)\n",
        "print(w_initial)\n",
        "for k in range(len(r)):\n",
        "  w_exe=w_initial\n",
        "  def mse(x,t):\n",
        "    def error(w,r):\n",
        "      return np.sum((np.dot(x,w)-t)*(np.dot(x,w)-t)*0.5)/len(x)+r[k]*0.5*np.linalg.norm(w,1)+r[k]*0.5*np.sum(w*w)\n",
        "    return error \n",
        "  GD = grad(mse(x_train,t_train), (0))\n",
        "  for i in range(iterations):\n",
        "    w_gradient = GD(w_exe,r)\n",
        "    w_exe-=learningrate*w_gradient\n",
        "    aggerr=np.sum((np.dot(x_train,w_exe)-t_train)*(np.dot(x_train,w_exe)-t_train)*0.5)/len(x_train)+r[k]*0.5*np.linalg.norm(w_exe,1)+r[k]*0.5*np.sum(w_exe*w_exe)\n",
        "    if i%10==0:\n",
        "      print(\"iteration: \",i,\"loss: \",aggerr)\n",
        "  print(\"learning done\")\n",
        "  y_val=x_val.dot(w_exe)\n",
        "  valerr=math.sqrt(np.sum((y_val-t_val)*(y_val-t_val)))\n",
        "  print(\"validaiton error: \",valerr)\n",
        "#pick the best performing model\n",
        "r=0.001\n",
        "w_exe=w_initial\n",
        "def mse(x,t):\n",
        "    def error(w,r):\n",
        "      return np.sum((np.dot(x,w)-t)*(np.dot(x,w)-t)*0.5)/len(x)+r*0.5*np.linalg.norm(w,1)+r*0.5*np.sum(w*w)\n",
        "    return error \n",
        "GD = grad(mse(x_train,t_train), (0))\n",
        "for i in range(iterations):\n",
        "  w_gradient = GD(w_exe,r)\n",
        "  w_exe-=learningrate*w_gradient\n",
        "  aggerr=np.sum((np.dot(x_train,w_exe)-t_train)*(np.dot(x_train,w_exe)-t_train)*0.5)/len(x_train)+r*0.5*np.linalg.norm(w_exe,1)+r*0.5*np.sum(w_exe*w_exe)\n",
        "  if i%10==0:\n",
        "    print(\"iteration: \",i,\"loss: \",aggerr)\n",
        "print(\"learning done\")\n",
        "y_val=x_val.dot(w_exe)\n",
        "valerr=math.sqrt(np.sum((y_val-t_val)*(y_val-t_val)))\n",
        "print(\"validaiton error: \",valerr)\n",
        "w_optimal=w_exe\n",
        "y_test=x_test.dot(w_optimal)\n",
        "testerr=math.sqrt(np.sum((y_test-t_test)*(y_test-t_test)))\n",
        "print(testerr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.83079386 0.1660409  0.45611215 0.6428884  0.56865394]\n",
            "iteration:  0 loss:  7.8751106\n",
            "iteration:  10 loss:  1.0536957\n",
            "iteration:  20 loss:  0.39844126\n",
            "iteration:  30 loss:  0.27517712\n",
            "iteration:  40 loss:  0.24323499\n",
            "iteration:  50 loss:  0.23420286\n",
            "iteration:  60 loss:  0.23158887\n",
            "iteration:  70 loss:  0.23082466\n",
            "iteration:  80 loss:  0.23059946\n",
            "iteration:  90 loss:  0.2305325\n",
            "learning done\n",
            "validaiton error:  0.6540044558122481\n",
            "iteration:  0 loss:  7.7157803\n",
            "iteration:  10 loss:  0.88817\n",
            "iteration:  20 loss:  0.20580508\n",
            "iteration:  30 loss:  0.07550642\n",
            "iteration:  40 loss:  0.041013174\n",
            "iteration:  50 loss:  0.031029657\n",
            "iteration:  60 loss:  0.028071761\n",
            "iteration:  70 loss:  0.027186897\n",
            "iteration:  80 loss:  0.026920294\n",
            "iteration:  90 loss:  0.026839398\n",
            "learning done\n",
            "validaiton error:  0.4345829308210187\n",
            "iteration:  0 loss:  7.699803\n",
            "iteration:  10 loss:  0.8714388\n",
            "iteration:  20 loss:  0.18624085\n",
            "iteration:  30 loss:  0.05518902\n",
            "iteration:  40 loss:  0.020417443\n",
            "iteration:  50 loss:  0.010327679\n",
            "iteration:  60 loss:  0.007330069\n",
            "iteration:  70 loss:  0.006430606\n",
            "iteration:  80 loss:  0.0061586397\n",
            "iteration:  90 loss:  0.0060757534\n",
            "learning done\n",
            "validaiton error:  0.4333429900187673\n",
            "iteration:  0 loss:  7.699803\n",
            "iteration:  10 loss:  0.8714388\n",
            "iteration:  20 loss:  0.18624085\n",
            "iteration:  30 loss:  0.05518902\n",
            "iteration:  40 loss:  0.020417443\n",
            "iteration:  50 loss:  0.010327679\n",
            "iteration:  60 loss:  0.007330069\n",
            "iteration:  70 loss:  0.006430606\n",
            "iteration:  80 loss:  0.0061586397\n",
            "iteration:  90 loss:  0.0060757534\n",
            "learning done\n",
            "validaiton error:  0.4333429900187673\n",
            "0.6156560028325071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hTRuzINYcwz"
      },
      "source": [
        "## 3.7 Report what you find\n",
        "\n",
        "How does the weight and bias compare under each regularization scheme? Which regularizer does the best on the test set?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWvoYKDGU5Bt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4EfhAAht8qH"
      },
      "source": [
        "Report your finding here: All 4 regularization scheme output very same test error under their most prefered model."
      ]
    }
  ]
}